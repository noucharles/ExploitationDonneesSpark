package fr.binome;

import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import fr.binome.functions.FilterVille;
import fr.binome.reader.CsvFileReader;
import fr.binome.writer.ResultWriter;
import lombok.extern.slf4j.Slf4j;
import org.apache.spark.SparkConf;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

/**
 * Hello world!
 *
 */
@Slf4j
public class MainApp {

    // DEVOIR FRANCK & CHARLES
    public static void main( String[] args ) {


        Config config = ConfigFactory.load();

        String inputPathStr = config.getString("3il.path.input");
        String outputPathStr = config.getString("3il.path.output");

        log.info( "Hello Count Words!");
        SparkConf sparkConf= new SparkConf().setMaster("local[2]").setAppName("SparkAppHospital");
        SparkSession sparkSession= SparkSession.builder().config(sparkConf).getOrCreate();
        CsvFileReader reader = new CsvFileReader(inputPathStr,sparkSession);


        Dataset<Row> dt= reader.get();

        Dataset<Row> ds = FilterVille.builder().ville("Limoges").build().apply(dt);
        ds.show(false);

//        Dataset<Row> ds = GroupByAddressePostal.builder().build().apply(lines);
//        ds.show(false);

        //lines.printSchema();
        long nblines = dt.count();
        log.info("nblines={}", nblines);
        /*ResultWriter resultWriter = new ResultWriter();
        long nb= dt.count();
        resultWriter.accept((int) nb);*/

        //log.info("nblines = {}",nb);

        ResultWriter resultWriter = new ResultWriter(outputPathStr);
        resultWriter.accept(ds);
        try {
            Thread.sleep(1000*60*10);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

    }
}
